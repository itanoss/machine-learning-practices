{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10 - NN, ReLU, Xavier, Dropout and Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /var/folders/3z/zvzt19xs7k955z3mjvlnm7sc0000gn/T/tmpgy2ui0l9/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /var/folders/3z/zvzt19xs7k955z3mjvlnm7sc0000gn/T/tmpgy2ui0l9/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /var/folders/3z/zvzt19xs7k955z3mjvlnm7sc0000gn/T/tmpgy2ui0l9/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Extracting /var/folders/3z/zvzt19xs7k955z3mjvlnm7sc0000gn/T/tmpgy2ui0l9/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy, os, urllib, gzip, tempfile, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "VALIDATION_SIZE = 5000\n",
    "\n",
    "def _read32(bytestream):\n",
    "    dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n",
    "    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
    "\n",
    "def download(filename, workdir):\n",
    "    if not os.path.exists(workdir):\n",
    "        os.mkdir(workdir)\n",
    "\n",
    "    filepath = os.path.join(workdir, filename)\n",
    "\n",
    "    print('Downloading', SOURCE_URL + filename)\n",
    "    urllib.request.urlretrieve(SOURCE_URL + filename, filename=filepath)\n",
    "    return filepath\n",
    "\n",
    "def extract_images(filepath):\n",
    "    print('Extracting', filepath)\n",
    "    with gzip.open(filepath) as bytestream:\n",
    "        magic = _read32(bytestream)\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Invalid magic number %d in MNIST image file: %s' % (magic, filepath))\n",
    "        num_images = _read32(bytestream)\n",
    "        rows = _read32(bytestream)\n",
    "        cols = _read32(bytestream)\n",
    "        buf = bytestream.read(rows * cols * num_images)\n",
    "        data = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "        data = data.reshape(num_images, rows, cols, 1)\n",
    "\n",
    "        # Reshape for NN\n",
    "        data = data.reshape(data.shape[0], data.shape[1] * data.shape[2])\n",
    "        data = data.astype(numpy.float32)\n",
    "        data = numpy.multiply(data, 1. / 255.)\n",
    "        return data\n",
    "\n",
    "def extract_labels(filepath):\n",
    "    print('Extracting', filepath)\n",
    "    with gzip.open(filepath) as bytestream:\n",
    "        magic = _read32(bytestream)\n",
    "        if magic != 2049:\n",
    "            raise ValueError('Invalid magic number %d in MNIST image file: %s' % (magic, filepath))\n",
    "        num_items = _read32(bytestream)\n",
    "        buf = bytestream.read(num_items)\n",
    "        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "\n",
    "        num_labels = labels.shape[0]\n",
    "        num_classes = 10    # 0..9\n",
    "        index_offset = numpy.arange(num_labels) * num_classes\n",
    "        labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "        labels_one_hot.flat[index_offset + labels.ravel()] = 1\n",
    "        return labels_one_hot\n",
    "\n",
    "workdir = tempfile.mkdtemp()\n",
    "train_images = extract_images(download(TRAIN_IMAGES, workdir))\n",
    "train_labels = extract_labels(download(TRAIN_LABELS, workdir))\n",
    "test_images = extract_images(download(TEST_IMAGES, workdir))\n",
    "test_labels = extract_labels(download(TEST_LABELS, workdir))\n",
    "validation_images = train_images[:VALIDATION_SIZE]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_images = train_images[VALIDATION_SIZE:]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "\n",
    "assert len(train_labels) == len(train_images)\n",
    "assert len(validation_labels) == len(validation_images)\n",
    "assert len(test_labels) == len(test_images)\n",
    "num_train_data = len(train_labels)\n",
    "num_validation_data = len(validation_labels)\n",
    "num_test_data = len(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 , cost =  4.849802413\n",
      "Epoch: 0002 , cost =  1.626679888\n",
      "Epoch: 0003 , cost =  1.096894575\n",
      "Epoch: 0004 , cost =  0.883146428\n",
      "Epoch: 0005 , cost =  0.762786742\n",
      "Epoch: 0006 , cost =  0.683292284\n",
      "Epoch: 0007 , cost =  0.625944211\n",
      "Epoch: 0008 , cost =  0.582150302\n",
      "Epoch: 0009 , cost =  0.547327469\n",
      "Epoch: 0010 , cost =  0.518842767\n",
      "Epoch: 0011 , cost =  0.495057617\n",
      "Epoch: 0012 , cost =  0.474860831\n",
      "Epoch: 0013 , cost =  0.457458310\n",
      "Epoch: 0014 , cost =  0.442271223\n",
      "Epoch: 0015 , cost =  0.428871944\n",
      "Learning finished\n",
      "Accuracy:  0.9009\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = .001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])    # image of shape 28 x 28 = 784\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])     # 0..9 digits\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(num_train_data / batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs = train_images[i*batch_size:(i+1)*batch_size]\n",
    "        batch_ys = train_labels[i*batch_size:(i+1)*batch_size]\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y:batch_ys})\n",
    "        avg_cost += c/ total_batch\n",
    "    \n",
    "    print('Epoch:', '{:04d}'.format(epoch + 1), ', cost = ', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy: ', sess.run(accuracy, feed_dict={X: test_images, Y: test_labels}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 , cost =  166.226272164\n",
      "Epoch: 0002 , cost =  41.136390598\n",
      "Epoch: 0003 , cost =  26.349806695\n",
      "Epoch: 0004 , cost =  18.620440508\n",
      "Epoch: 0005 , cost =  13.632632707\n",
      "Epoch: 0006 , cost =  10.225308105\n",
      "Epoch: 0007 , cost =  7.779223825\n",
      "Epoch: 0008 , cost =  5.863761199\n",
      "Epoch: 0009 , cost =  4.535676005\n",
      "Epoch: 0010 , cost =  3.435842434\n",
      "Epoch: 0011 , cost =  2.604721716\n",
      "Epoch: 0012 , cost =  1.997740422\n",
      "Epoch: 0013 , cost =  1.537656595\n",
      "Epoch: 0014 , cost =  1.135337872\n",
      "Epoch: 0015 , cost =  0.955357635\n",
      "Learning finished\n",
      "Accuracy:  0.9433\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])    # image of shape 28 x 28 = 784\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])     # 0..9 digits\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(num_train_data / batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs = train_images[i*batch_size:(i+1)*batch_size]\n",
    "        batch_ys = train_labels[i*batch_size:(i+1)*batch_size]\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y:batch_ys})\n",
    "        avg_cost += c/ total_batch\n",
    "    \n",
    "    print('Epoch:', '{:04d}'.format(epoch + 1), ', cost = ', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy: ', sess.run(accuracy, feed_dict={X: test_images, Y: test_labels}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 , cost =  0.336163099\n",
      "Epoch: 0002 , cost =  0.134684066\n",
      "Epoch: 0003 , cost =  0.086793265\n",
      "Epoch: 0004 , cost =  0.059878554\n",
      "Epoch: 0005 , cost =  0.042300234\n",
      "Epoch: 0006 , cost =  0.032108150\n",
      "Epoch: 0007 , cost =  0.024569017\n",
      "Epoch: 0008 , cost =  0.021572553\n",
      "Epoch: 0009 , cost =  0.018631316\n",
      "Epoch: 0010 , cost =  0.016060304\n",
      "Epoch: 0011 , cost =  0.013056713\n",
      "Epoch: 0012 , cost =  0.011068924\n",
      "Epoch: 0013 , cost =  0.010429467\n",
      "Epoch: 0014 , cost =  0.013630596\n",
      "Epoch: 0015 , cost =  0.011559048\n",
      "Learning finished\n",
      "Accuracy:  0.9732\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = .001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])    # image of shape 28 x 28 = 784\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])     # 0..9 digits\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(num_train_data / batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs = train_images[i*batch_size:(i+1)*batch_size]\n",
    "        batch_ys = train_labels[i*batch_size:(i+1)*batch_size]\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y:batch_ys})\n",
    "        avg_cost += c/ total_batch\n",
    "    \n",
    "    print('Epoch:', '{:04d}'.format(epoch + 1), ', cost = ', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy: ', sess.run(accuracy, feed_dict={X: test_images, Y: test_labels}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dropout for MNIST\n",
    "To prevent overfitting\n",
    "\n",
    "# `keep_prob`\n",
    "* Train: 0.5 ~ 0.7\n",
    "* Testing: obviously 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 , cost =  0.426693615\n",
      "Epoch: 0002 , cost =  0.161900063\n",
      "Epoch: 0003 , cost =  0.121908065\n",
      "Epoch: 0004 , cost =  0.100050659\n",
      "Epoch: 0005 , cost =  0.082410858\n",
      "Epoch: 0006 , cost =  0.073014813\n",
      "Epoch: 0007 , cost =  0.066355426\n",
      "Epoch: 0008 , cost =  0.058996734\n",
      "Epoch: 0009 , cost =  0.057059867\n",
      "Epoch: 0010 , cost =  0.049955109\n",
      "Epoch: 0011 , cost =  0.047349964\n",
      "Epoch: 0012 , cost =  0.047509635\n",
      "Epoch: 0013 , cost =  0.040289384\n",
      "Epoch: 0014 , cost =  0.040143691\n",
      "Epoch: 0015 , cost =  0.038205221\n",
      "Learning finished\n",
      "Accuracy:  0.9807\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "learning_rate = .001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])    # image of shape 28 x 28 = 784\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])     # 0..9 digits\n",
    "\n",
    "W1 = tf.get_variable('W1', shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable('W2', shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable('W3', shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable('W4', shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L3, W4) + b4\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(num_train_data / batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs = train_images[i*batch_size:(i+1)*batch_size]\n",
    "        batch_ys = train_labels[i*batch_size:(i+1)*batch_size]\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y:batch_ys, keep_prob: 0.7})\n",
    "        avg_cost += c/ total_batch\n",
    "    \n",
    "    print('Epoch:', '{:04d}'.format(epoch + 1), ', cost = ', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy: ', sess.run(accuracy, feed_dict={X: test_images, Y: test_labels, keep_prob: 1}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "https://www.tensorflow.org/api_guides/python/train : List of optimizers\n",
    "\n",
    "* ADAM : a method for stochastic optimization, most recommended"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
