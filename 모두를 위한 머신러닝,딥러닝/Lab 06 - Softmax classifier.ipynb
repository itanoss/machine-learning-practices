{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax function\n",
    "=====\n",
    "Logistic classifier: `WX = Y`\n",
    "Scores -> Probabilities : `S(y_i) = e^(y_i) / sum( e^(y_j) )`\n",
    "\n",
    "Cross-entropy cost function\n",
    "-----\n",
    "`D(S, L) = -sum( L_i * log(S_i) )`\n",
    " - `S(Y) = Y_hat`\n",
    " - `L = Y`\n",
    " \n",
    "`Loss = 1/N sum( D(S(W*X_i+b), Li) )`\n",
    "\n",
    "Gradient descent\n",
    "-----\n",
    "STEP = `-alpha * derivative( loss(w1, w2) )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.09151 [[ -3.33333330e-04   4.16666626e-05   2.91666656e-04]\n",
      " [ -1.45833357e-03   2.91666773e-04   1.16666686e-03]\n",
      " [ -1.45833357e-03   4.16666706e-04   1.04166684e-03]]\n",
      "200 0.780599 [[-0.03302823  0.002706    0.03032222]\n",
      " [-0.1265296   0.03458844  0.0919412 ]\n",
      " [-0.1269189   0.05814527  0.06877366]]\n",
      "400 0.739866 [[-0.04844064  0.00034499  0.04809566]\n",
      " [-0.17510462  0.04989799  0.12520666]\n",
      " [-0.17469139  0.09413347  0.08055794]]\n",
      "600 0.721218 [[-0.05942659 -0.00417597  0.06360256]\n",
      " [-0.20712054  0.05753902  0.1495816 ]\n",
      " [-0.20532399  0.12033778  0.08498629]]\n",
      "800 0.709686 [[-0.06820342 -0.00972322  0.07792662]\n",
      " [-0.23153698  0.06208384  0.16945331]\n",
      " [-0.22810657  0.14162229  0.08648437]]\n",
      "1000 0.701507 [[-0.07561301 -0.01586152  0.0914745 ]\n",
      " [-0.25150681  0.06512281  0.18638413]\n",
      " [-0.24633147  0.15978576  0.08654573]]\n",
      "1200 0.695243 [[-0.08207646 -0.02237551  0.10445193]\n",
      " [-0.26852417  0.06736121  0.20116307]\n",
      " [-0.26156002  0.17570314  0.08585697]]\n",
      "1400 0.690207 [[-0.08783827 -0.02914042  0.11697869]\n",
      " [-0.28342199  0.06916226  0.21425985]\n",
      " [-0.27466068  0.18988335  0.08477752]]\n",
      "1600 0.68602 [[-0.09305493 -0.03607642  0.12913133]\n",
      " [-0.29671517  0.07072964  0.22598557]\n",
      " [-0.28616807  0.20265807  0.08351023]]\n",
      "1800 0.682453 [[-0.09783325 -0.04312892  0.14096202]\n",
      " [-0.30874595  0.07218322  0.23656273]\n",
      " [-0.29643613  0.21426135  0.08217476]]\n",
      "2000 0.679356 [[-0.10224988 -0.0502587   0.15250845]\n",
      " [-0.31975394  0.07359526  0.24615869]\n",
      " [-0.30571026  0.22486752  0.08084275]]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1., 2., 1.],\n",
    "          [1., 3., 2.],\n",
    "          [1., 3., 4.],\n",
    "          [1., 5., 5.],\n",
    "          [1., 7., 5.],\n",
    "          [1., 2., 5.],\n",
    "          [1., 6., 6.],\n",
    "          [1., 7., 7.]]\n",
    "y_data = [[0., 0., 1.],\n",
    "          [0., 0., 1.],\n",
    "          [0., 0., 1.],\n",
    "          [0., 1., 0.],\n",
    "          [0., 1., 0.],\n",
    "          [0., 1., 0.],\n",
    "          [0., 0., 1.],\n",
    "          [0., 0., 1.]]\n",
    "# x_data = [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "#           [2., 3., 3., 5., 7., 2., 6., 7.],\n",
    "#           [1., 2., 4., 5., 5., 5., 6., 7.]]\n",
    "# y_data = [[0., 0., 0., 0., 0., 0., 1., 1.],\n",
    "#           [0., 0., 0., 1., 1., 1., 0., 0.],\n",
    "#           [1., 1., 1., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])  # x1, x2 and 1 (for bias)\n",
    "Y = tf.placeholder(\"float\", [None, 3])  # A, B, C => 3 classes\n",
    "W = tf.Variable(tf.zeros([3, 3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W))\n",
    "\n",
    "learning_rate = .001\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), reduction_indices=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in xrange(2001):\n",
    "    sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 200 == 0:\n",
    "        print step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.67555248e-05   2.51073748e-01   7.48849511e-01]] [2]\n",
      "[[ 0.01594904  0.45657307  0.52747792]] [2]\n",
      "[[ 0.2069089   0.3229892   0.47010192]] [2]\n",
      "[[  7.67555248e-05   2.51073748e-01   7.48849511e-01]\n",
      " [  1.59490425e-02   4.56573069e-01   5.27477920e-01]\n",
      " [  2.06908897e-01   3.22989196e-01   4.70101923e-01]] [2 2 2]\n"
     ]
    }
   ],
   "source": [
    "a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7]]})\n",
    "print a, sess.run(tf.arg_max(a, 1))\n",
    "\n",
    "b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4]]})\n",
    "print b, sess.run(tf.arg_max(b, 1))\n",
    "\n",
    "c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0]]})\n",
    "print c, sess.run(tf.arg_max(c, 1))\n",
    "\n",
    "all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7], [1, 3, 4], [1, 1, 0]]})\n",
    "print all, sess.run(tf.arg_max(all, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
